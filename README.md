# DataScience-AI-ML Portfolio
This is a collection of projects related to DataScience, Machine Learning and AI done for academic and self-learning purposes.
<br />
This is updated on regular basis.

- **Email**: [muhammadahmedsuhail@gmail.com](muhammadahmedsuhail@gmail.com)
- **LinkedIn**: [linkedin.com/muhammadahmedsuhail](https://www.linkedin.com/in/muhammad-ahmed-suhail/)

## Projects

<img align="left" width="250" height="150" src="https://user-images.githubusercontent.com/72251313/234053103-3591fa28-5a6a-40f4-b32c-07decc933c0d.jpg"> **[EDA on Pakistan Floods](https://github.com/MuhammadAhmedSuhail/EDA-on-Pakistan-Floods)**

This project aims to perform exploratory data analysis (EDA) using visualizations on the floods in Pakistan. The goal is to scrape data, extract insights and patterns from the flood data and create meaningful visualizations to better understand the situation. The project involves data scraping, data preprocessing, and data visualization using Python libraries like Selenium matplotlib and Seaborn.

<br/>

<img align="left" width="250" height="150" src="https://user-images.githubusercontent.com/72251313/234058605-0fc946b5-8c2c-43e5-b925-eaad154f010d.jpg"> **[EDA on DBLP using Python and MongoDB](https://github.com/MuhammadAhmedSuhail/EDA-on-DBLP-using-Python-MongoDB)**

The purpose of this project is to perform exploratory data analysis (EDA) on the DBLP computer science bibliography. By analyzing the bibliographic information on major computer science journals and proceedings, I hope to gain insights into the trends and patterns in computer science research over time. I will use MongoDB to store and manage the data, and use pandas, matplotlib and Seaborn to generate visualizations.

<br/>

<img align="left" width="250" height="150" src="https://user-images.githubusercontent.com/72251313/234069116-24f3f705-a61e-4da6-874e-6a51bb56d03c.jpg"> **[WebScraping Ecommerce Website](https://github.com/MuhammadAhmedSuhail/WebScraping-Ecommerce-Website)**

This project is a web scraping tool that extracts data from the e-commerce website Daraz.pk. The tool can scrape reviews of a single product page, search for products using a keyword search and return the top 80 elements for that keyword, and extract products on a flash sale and return the product name, price, discounted price, top 3 reviews, and rating.

<br/>

<img align="left" width="250" height="150" src="https://user-images.githubusercontent.com/72251313/234058926-bb393623-4098-4cf8-b806-1e89f525937b.jpg"> **[Weather Data Analysis using Python](https://github.com/MuhammadAhmedSuhail/Weather-Data-Analysis-using-Python)**

The aim of this project is to perform a comprehensive analysis of weather data using Python. Weather data provides us with information about temperature, humidity, wind speed, and other atmospheric conditions. This project also aims to address the common challenge of missing data in data analysis. The dataset used is "Weather Records.xlsx", which contains 13 columns, including a prediction column for precipitation (in).

<br/>

<img align="left" width="250" height="150" src="https://user-images.githubusercontent.com/72251313/234054538-a87b143b-8f26-471f-802f-1482f75ffc36.jpeg"> **[Customer Churn Analysis](https://github.com/MuhammadAhmedSuhail/Churn-Analysis-using-R)**

In today’s retail industry, understanding customers better is critical for businesses to stay competitive and profitable. Retail data is growing exponentially in variety, volume, velocity & value with each passing year. Smart retailers are well aware that this data can be utilized and eventually holds the prospective for profit. As a result, retailers are becoming more conscious about utilization of data and information kept in their repositories, so they can integrate and analyze these large volumes of data to come up with results that can support the quality of their decision-making, in order to stay at a competitive advantage and to increase profits. This project analyzes a dataset provided by a food store in Pakistan dated between September 17, 2014, and October 26, 2014 (6 weeks approx.) to understand customer churn behavior. The dataset involves only three columns, and each row represents the purchasing done by customers in each day.

<br/>

<img align="left" width="250" height="150" src="https://user-images.githubusercontent.com/72251313/234059798-d9be87d3-fe50-4355-a1b7-706e94487262.png"> **[Retail Analysis using R](https://github.com/MuhammadAhmedSuhail/Retail-Analysis-using-R)**

In today’s retail industry, understanding customers better is critical for businesses to stay competitive and profitable. Retail data is growing exponentially in variety, volume, velocity & value with each passing year. Smart retailers are well aware that this data can be utilized and eventually holds the prospective for profit. As a result, retailers are becoming more conscious about utilization of data and information kept in their repositories, so they can integrate and analyze these large volumes of data to come up with results that can support the quality of their decision-making, in order to stay at a competitive advantage and to increase profits. This project analyzes a dataset provided by a food store in Pakistan dated between September 17, 2014, and October 26, 2014 (6 weeks approx.) to understand customer churn behavior. The dataset involves only three columns, and each row represents the purchasing done by customers in each day.

<br/>

<img align="left" width="250" height="150" src="https://user-images.githubusercontent.com/72251313/234060277-c89301f3-089d-49d1-a862-fa14be91fec3.png"> **[Admissions Analysis using R](https://github.com/MuhammadAhmedSuhail/Admissions-Analysis-using-R)**

This project uses R to analyze the actual data of NUCES 2018 admissions. Each row corresponds to one student. The dataset includes various features such as HSSC marks, Matric marks, city of residence, and more. The goal of this project is to answer several questions about the dataset and gain insights into the admissions process.

<br/>

<img align="left" width="250" height="150" src="https://user-images.githubusercontent.com/72251313/234060965-42b1947c-7797-42d4-a006-948939a5e1f9.png"> **[Relation Analysis R](https://github.com/MuhammadAhmedSuhail/Relation-Analysis-R)**

This project aims to analyze the admissions in a university using R programming language. The focus of this project is to test the relation between various variables such as SSC Marks Obtained, NU_BS_Test Marks, and HSSC Marks Obtained. The project also involves creating residual plots and calculating r2 values to confirm the results obtained.

<br/>

<img align="left" width="250" height="150" src="https://user-images.githubusercontent.com/72251313/233680766-03f697a8-e951-4214-80a9-18990076d61c.png"> **[Near Real-Time DataWarehouse Analysis](https://github.com/MuhammadAhmedSuhail/Near-Real-Time-DataWarehouse-Analysis)**

The objective of this project is to design, implement, and analyze a near-real-time Data Warehouse (DW) prototype for METRO Shopping Store in Pakistan. As one of the biggest superstore chains in Pakistan, METRO has thousands of customers, and it is important for the store to analyze the shopping behavior of their customers in real-time. Based on that, the store can optimize their selling strategies, such as giving promotions on different products.

<br/>

<img align="left" width="250" height="150" src="https://user-images.githubusercontent.com/72251313/234061764-feb363ee-6bbb-4b68-847e-16f8f512d194.jpg"> **[MapReduce DBLP using Hadoop Framework](https://github.com/MuhammadAhmedSuhail/MapReduce-Hadoop-DBLP)**

This project implements a MapReduce algorithm in Java to find the number of articles published in each journal per year from the DBLP articles dataset. The input to the MapReduce program is a XML file.The dataset contains bibliographic information about articles, including the journal/book title, authors, year, etc. We are interested in the number of articles published in each journal per year. The MapReduce algorithm consists of two stages: the map stage and the reduce stage.

<br/>

<img align="left" width="250" height="150" src="https://user-images.githubusercontent.com/72251313/234063382-66194813-da88-4d81-834b-2b3cf0feb3b1.png"> **[Text Normalization NLTK](https://github.com/MuhammadAhmedSuhail/Text-Normalization-NLTK)**

This project implements text normalization using the Natural Language Toolkit (NLTK) library. Text normalization is the process of converting text into a canonical (standard) form. The project uses various normalization techniques such as stemming, lemmatization, and removing stop words to preprocess text data. The project also implements specific rules to normalize certain types of text such as URLs, dates, and phone numbers. The langauge of the Text is Roman-Urdu.

<br/>

<img align="left" width="250" height="150" src="https://user-images.githubusercontent.com/72251313/234063934-858b564e-e9b7-4d5a-9187-caa4efdcdd32.jpg"> **[Apriori Algorithm and Multiple Minimum Support](https://github.com/MuhammadAhmedSuhail/Apriori-Algorithm-and-Multiple-Support)**

This project involves the implementation of the Apriori algorithm in Python from scratch and extending it to handle multiple minimum support thresholds simultaneously. The project also involves the use of data visualization techniques to explore and analyze the frequent itemsets generated by the Apriori implementation. The maximum value for k in both algorithms will be 5.

<br/>

<img align="left" width="250" height="150" src="https://user-images.githubusercontent.com/72251313/234062912-3faf306a-f59e-411f-9cc8-d14afa0d7c64.png"> **[House Price Prediction](https://github.com/MuhammadAhmedSuhail/House-Price-Prediction)**

The problem statement for this project is to predict the final sale price of residential homes in Ames, Iowa using machine learning algorithms based on the explanatory variables in the dataset. The target variable is SalePrice, which represents the property's sale price in dollars.

<br/>

<img align="left" width="250" height="150" src="https://user-images.githubusercontent.com/72251313/234068915-d8fbaedb-fea0-4778-aff4-0b9840db7e95.png"> **[Kafka ML Classification App](https://github.com/MuhammadAhmedSuhail/Kafka-ML_Classification-App)**

This project is aimed at collecting and classifying data from a mobile phone's accelerometer and gyroscope sensors. The project is divided into three main parts Data collection, Data classification and Frontend implementation. The data collection involves creating a mobile app that collects data from the phone's sensors and stores it in a database. The data classification involves processing the data using machine learning classification models. The frontend implementation involves displaying live data from the phone and predicting the position or state of the phone based on the labeled data.

<br/>

<img align="left" width="250" height="150" src="https://user-images.githubusercontent.com/72251313/234064804-e90402be-4905-4bc4-99dd-3074c50eef33.png"> **[RUHA Voice Assistant](https://github.com/MuhammadAhmedSuhail/RUHA-Voice-Assistant)**

This project aims to train 5 different classifiers on the RUHA dataset using the scikit-learn library. The dataset is an audio dataset, and I will explore different machine learning algorithms and techniques to train the classifiers. After training the classifiers, I will evaluate their performance by displaying the confusion matrix, accuracy, recall, precision, and F1-measure. The most important part of the project is to create a web application using Flask, a Python web framework. I will deploy our project on the web application, where users can interact with it by providing input and getting the output. For example, a user can input "Ruha lights band kardo" on the website, and the 5 trained classifiers will predict the result, which will be creatively displayed on the webpage. After displaying the result, I will provide a result analytics button that will redirect the user to another webpage, where the complete result will be displayed, including how the 5 classifiers performed on the dataset.

<br/>

<img align="left" width="250" height="150" src="https://user-images.githubusercontent.com/72251313/234065807-2441a4e7-186b-483c-aadc-5021ad0bd13f.jpg"> **[Face Recognition Using KNN](https://github.com/MuhammadAhmedSuhail/Face-Recognition-using-KNN)**

In this project, I used a simplified version of the CMU Pose, Illumination, and Expression (PIE) Dataset to implement a k-Nearest Neighbors (k-NN) classifier for face recognition. The dataset consisted of 10 subjects spanning five near-frontal poses, and there were 170 images for each individual. In addition, all the images were resized to 32x32 pixels. The dataset was provided in the form of a CSV file with 1700 rows and 1024 columns. Each row was an instance and each column a feature. The first 170 instances belonged to the first subject, the next 170 to the second subject, and so on.

<br/>

<img align="left" width="250" height="150" src="https://user-images.githubusercontent.com/72251313/234066497-aa469134-e70f-418a-85e4-a3dbd002f3c9.png"> **[Foreground Segmentation using K-Means](https://github.com/MuhammadAhmedSuhail/Foreground-Segmentation-using-K-Means)**

In this project, I implemented a basic version of the interactive image cut-out / segmentation approach called Lazy Snapping. I was given several test images along with corresponding auxiliary images depicting the foreground and background **seed** pixels marked with red and blue brush-strokes, respectively. My program exploited these partial human annotations to compute a precise figure-ground segmentation.

<br/>

<img align="left" width="250" height="150" src="https://user-images.githubusercontent.com/72251313/234067232-4a5cb86f-58ea-4950-a639-65f4eb8195df.jpg"> **[Clashless-TimeTable-using-AI](https://github.com/MuhammadAhmedSuhail/Clashless-TimeTable-using-AI)**

This project aims to generate a clash-free timetable for a given set of courses and rooms using a Genetic Algorithm. The algorithm takes a timetable in CSV format as input, preprocesses the data to get the number of rooms, all courses, and all time slots, and then generates a new timetable that satisfies certain constraints, such as avoiding course clashes no section having multiple courses at the same time and no teacher having multiple classes at the same time.

<br/>

<img align="left" width="250" height="150" src="https://user-images.githubusercontent.com/72251313/234068190-571b165f-1df5-4731-a809-90539705d647.png"> **[Image Reconstruction using A* Algorithm](https://github.com/MuhammadAhmedSuhail/Image-Reconstruction-using-AI)**

In this project, I implemented the A* algorithm to solve the problem of image reconstruction. My task was to reconstruct a 512x512 image that had been divided into 16x16 boxes and shuffled. To accomplish this, I created a Python program that takes input as the shuffled image and uses the A* algorithm to determine the optimal sequence of moves to reconstruct the original image. I used the skimage library to display the shuffled and reconstructed images side by side.

<br/>

## Core Competencies

- **Methodologies**: Machine Learning, Time Series Analysis, Natural Language Processing, Advanced Statistics, Explainable AI, Big Data Analytics, Exploratory Data Analysis, Data Preprocessing
- **Languages**: Python (Pandas, Numpy, Scikit-Learn, Seaborn, Matplotlib), R (Dplyr, Ggplot2), SQL, C++, Java
- **Tools**: MySQL, Tableau, Git, Flask, MS Excel, VS Code, R Studio, Eclipse
